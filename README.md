# simulation_of_a_dataset_from_descriptive_statistics
Simulation of a dataset based on descriptive statistical characteristics

![image](https://github.com/Praemuntiacus/Data_Analyst_Job_Market_2022/assets/125415799/125c7f3d-0d19-41d5-8988-8a6183a1d716)

In my work, I frequently encounter datasets that are only presented through statistical descriptions, such as sample size, mean, standard deviation, maximum, and minimum values. These limitations restrict the possibilities of data utilization, comparisons with other samples, and visualizations. In this project, I present my attempt to reconstruct numerical data samples from such statistical descriptions by generating random values assuming a normal distribution of the data. The goal is to approximate the original dataset and gain valuable insights despite the limited information available.

The method proposed here can be referred to as a "**simulation-based method for reconstructing a sample of numerical data assuming a normal distribution**". This method utilizes random number generation from a standard normal distribution and then applies transformations to match the desired mean, standard deviation, maximum, and minimum values.

By simulating random numbers and adjusting their characteristics, this method approximates a sample that follows a normal distribution with the specified parameters. However, it's important to note that **the reconstructed sample is an approximation** and may not precisely represent the original data that generated the provided summary statistics.

The set of data used in this study represents measurements of reindeer talus from several Paleolithic sites in France (**Kuntz, D., 2011**. Ostéométrie et migration(s) du renne (*Rangifer tarandus*) dans le Sud-Ouest de la France au cours du dernier Pléniglaciaire et du Tardiglaciaire (21 500-13 000 cal. BP) (Doctoral dissertation, Université Toulouse le Mirail-Toulouse II)).

To visualize missing data in the dataset, I utilized the **missingno** library. As evident from the **missinno** visualisation, the dataset used in this work is incomplete, containing missing values. To address this issue, I performed data imputation using multiple linear regression for each parameter that requires imputation.

Due to missing values in some individuals for both variables, the imputation process results in duplicates, with one missing value and one imputed value. To handle this issue effectively, I performed a group-by operation on all initially complete values (since the ID of individuals is missing) and used aggregation to select the first available value from the duplicates (.agg(‘first’)). This process effectively removes the duplicates while retaining the relevant data.
For the purpose of bivariate plotting, I sorted the majority of the data from smallest to largest values to capture the general correlation trend. However, a smaller portion of the data will remain unsorted to retain data dispersion.

I attempted to use the standard deviation to capture the dispersion of the reconstructed data (sorted_ratio), but my efforts were unsuccessful. Instead, I propose to derive the value of the sorted ratio from the linear equation of the reconstructed data scatter plot, which is adjusted to closely match the linear equation of the original data. This adjustment ensures that the reconstructed data aligns with the linear relationship observed in the original dataset.
To generate the data sample with the desired statistical characteristics, I created the "reconstruct_sample" function. This function takes the mean, standard deviation, maximum, minimum, and number of individuals as input parameters. It generates a sample of random numbers from a standard normal distribution using np.random.randn. Subsequently, the sample is rescaled and transformed to match the desired mean and standard deviation. Finally, the function adjusts the range of the sample to fit within the specified maximum and minimum values.

So, in the proposed code, np.random.randn(num_individuals) generates "num_individuals" random numbers from a standard normal distribution, where the mean is 0 and the standard deviation is 1. These random values are then rescaled and transformed to match the desired mean and standard deviation specified in the function arguments: "sample_r = sample_r * std_dev + mean." By multiplying each random value by the standard deviation and adding the mean, the sample is transformed to have the desired mean ("mean") and standard deviation ("std_dev").

This part of the code ensures that the generated sample follows an approximately normal distribution with the desired mean and standard deviation, assuming that the original data also follows a normal distribution with the provided statistics.

The reconstructed sample is returned by the function and stored in the "reconstructed_sample" object. The obtained statistical characteristics (mean and standard deviation) of the reconstructed sample are close to those of the initial data, but not identical with them.

The reconstruction of data based on descriptive statistics parameters provides an approximate representation of the initial data used for this exercise. However, it is important to acknowledge that the randomly generated reconstructed data may differ from the original data in terms of their distribution. Nevertheless, it is worth considering that the individuals in the initial data also possess inherent randomness. Consequently, we are comparing two datasets of random nature, which exhibit similar statistical characteristics. When interpreting the results, it is essential to recognize the limitations of the reconstruction process and the inherent variability present in both datasets.

![image](https://github.com/Praemuntiacus/Data_Analyst_Job_Market_2022/assets/125415799/1c09e9ef-5b0f-448a-b188-1aa46e343621)

Dans mon travail, je rencontre fréquemment des ensembles de données présentés uniquement à travers des descriptions statistiques telles que la taille de l'échantillon, la moyenne, l'écart-type, les valeurs maximales et minimales. Ces limitations restreignent les possibilités d'utilisation des données, les comparaisons avec d'autres échantillons et les visualisations. Dans ce projet, je présente ma tentative de reconstruire des échantillons de données numériques à partir de telles descriptions statistiques en générant des valeurs aléatoires en supposant une distribution normale des données. L'objectif est d'approximer l'ensemble de données original et d'acquérir des informations détaillés malgré les informations limitées disponibles.

La méthode proposée ici peut être appelée "méthode de simulation pour la reconstruction d'un échantillon de données numériques en supposant une distribution normale". Cette méthode utilise la génération de nombres aléatoires à partir d'une distribution normale standard, puis applique des transformations pour correspondre à la moyenne, l'écart-type, les valeurs maximales et minimales souhaitées.

En simulant des nombres aléatoires et en ajustant leurs caractéristiques, cette méthode approxime un échantillon qui suit une distribution normale avec les paramètres spécifiés. Cependant, il est important de noter que l'échantillon reconstruit est une approximation et peut ne pas représenter précisément les données originales qui ont généré les statistiques sommaires fournies.

L'ensemble de données utilisé dans cette étude représente des mesures de talus de rennes provenant de plusieurs sites paléolithiques en France (Kuntz, D., 2011. Ostéométrie et migration(s) du renne (Rangifer tarandus) dans le Sud-Ouest de la France au cours du dernier Pléniglaciaire et du Tardiglaciaire (21 500-13 000 cal. BP) (Thèse de doctorat, Université Toulouse le Mirail-Toulouse II)).

Pour visualiser les données manquantes dans l'ensemble de données, j'ai utilisé la bibliothèque "missingno". Comme le montre la visualisation de "missingno", l'ensemble de données utilisé dans ce travail est incomplet, contenant des valeurs manquantes. Pour remédier à ce problème, j'ai effectué une imputation de données en utilisant la régression linéaire multiple pour chaque paramètre nécessitant une imputation.

En raison des valeurs manquantes chez certains individus pour les deux variables, le processus d'imputation entraîne la présence de doublons, avec une valeur manquante et une valeur imputée. Pour gérer efficacement ce problème, j'ai réalisé une opération de regroupement (group-by) sur toutes les valeurs initialement complètes (étant donné que l'ID des individus est manquant) et j'ai utilisé l'agrégation pour sélectionner la première valeur disponible parmi les doublons (.agg('first')). Ce processus élimine efficacement les doublons tout en conservant les données pertinentes.

Dans le but de réaliser un diagramme bivarié, j'ai trié la majorité des données des plus petites aux plus grandes valeurs afin de capturer la tendance générale de corrélation. Cependant, une plus petite partie des données reste non triée afin de conserver la dispersion des données.

J'ai tenté d'utiliser l'écart-type pour capturer la dispersion des données reconstruites (sorted_ratio), mais mes efforts ont été infructueux. À la place, je propose de dériver la valeur du ratio trié à partir de l'équation linéaire du nuage de points des données reconstruites, ajustée pour correspondre étroitement à l'équation linéaire des données originales. Cet ajustement garantit que les données reconstruites sont en accord avec la relation linéaire observée dans l'ensemble de données d'origine.

Pour générer l'échantillon de données avec les caractéristiques statistiques souhaitées, j'ai créé la fonction "reconstruct_sample". Cette fonction prend en entrée les paramètres suivants : la moyenne, l'écart-type, la valeur maximale, la valeur minimale et le nombre d'individus. Elle génère un échantillon de nombres aléatoires à partir d'une distribution normale standard en utilisant "np.random.randn". Ensuite, l'échantillon est mis à l'échelle et transformé pour correspondre à la moyenne et l'écart-type souhaités. Enfin, la fonction ajuste l'étendue de l'échantillon pour qu'elle corresponde aux valeurs maximale et minimale spécifiées.
Donc, dans le code proposé, "np.random.randn(num_individuals)" génère "num_individuals" nombres aléatoires à partir d'une distribution normale standard, où la moyenne est de 0 et l'écart-type est de 1. Ces valeurs aléatoires sont ensuite mises à l'échelle et transformées pour correspondre à la moyenne ("mean") et à l'écart-type ("std_dev") spécifiés dans les arguments de la fonction : "sample_r = sample_r * std_dev + mean". En multipliant chaque valeur aléatoire par l'écart-type et en ajoutant la moyenne, l'échantillon est transformé pour avoir la moyenne souhaitée ("mean") et l'écart-type souhaité ("std_dev").

Cette partie du code assure que l'échantillon généré suit une distribution approximativement normale avec la moyenne et l'écart-type souhaités, en supposant que les données originales suivent également une distribution normale avec les statistiques fournies.

L'échantillon reconstruit est renvoyé par la fonction et stocké dans l'objet "reconstructed_sample". Les caractéristiques statistiques obtenues (moyenne et écart-type) de l'échantillon reconstruit sont proches de celles des données initiales, mais pas identiques.

La reconstruction des données basée sur les paramètres de statistiques descriptives fournit une représentation approximative des données initiales utilisées pour cet exercice. Cependant, il est important de reconnaître que les données reconstruites générées de manière aléatoire peuvent différer des données originales en termes de distribution. Néanmoins, il est important de considérer que les individus dans les données initiales possèdent également une aléatorité inhérente. Par conséquent, nous comparons deux ensembles de données de nature aléatoire, qui présentent des caractéristiques statistiques similaires. Lors de l'interprétation des résultats, il est essentiel de reconnaître les limitations du processus de reconstruction et la variabilité inhérente présente dans les deux ensembles de données.
